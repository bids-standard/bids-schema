#!/usr/bin/env python3
"""Process BEP schemas by mapping them to their corresponding PRs.

Reads BEP definitions from bids-website and syncs schemas from PRs.
"""

import yaml
import json
import os
import sys
import shutil
import subprocess
import tempfile
from pathlib import Path
from functools import lru_cache


@lru_cache(maxsize=128)
def load_pr_metadata(pr_metadata_path: str) -> dict:
    """Load PR metadata with LRU caching to avoid reopening the same file multiple times.

    Args:
        pr_metadata_path: Path to PR_METADATA.json file

    Returns:
        Dict containing PR metadata, or empty dict if file doesn't exist
    """
    try:
        with open(pr_metadata_path, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}


def main():
    # Configuration
    script_dir = Path(__file__).parent.absolute()
    base_dir = script_dir.parent
    pr_dir = base_dir / "PRs"
    bep_dir = base_dir / "BEPs"
    website_repo = Path(os.environ.get('BIDS_WEBSITE_REPO', str(base_dir / "../bids-website")))

    # Ensure we're in the right directory
    os.chdir(base_dir)

    # Check if bids-website repo exists or needs to be cloned
    if not website_repo.exists():
        print(f"BIDS website repository not found at {website_repo}. Cloning...")
        with tempfile.TemporaryDirectory() as tmp_dir:
            website_repo = Path(tmp_dir) / "bids-website"
            subprocess.run(['git', 'clone', 'https://github.com/bids-standard/bids-website/', str(website_repo)], check=True)

    # Check if BEPs YAML file exists
    beps_file = website_repo / "data" / "beps" / "beps.yml"
    if not beps_file.exists():
        print(f"Error: BEPs file not found at {beps_file}")
        sys.exit(1)

    print(f"Processing BEPs from {beps_file}...")

    # Read BEPs YAML file
    with open(beps_file, 'r') as f:
        beps_data = yaml.safe_load(f)

    if not beps_data:
        print("No BEPs found in YAML file")
        return

    processed_count = 0
    synced_count = 0

    # Process each BEP
    for bep in beps_data:
        if not isinstance(bep, dict):
            continue

        bep_number = bep.get('number', '').lstrip('0')  # Remove leading zeros
        if not bep_number:
            continue

        title = bep.get('title', 'Untitled')
        pull_request = bep.get('pull_request', '')

        # Extract PR number from URL if present
        pr_number = None
        if pull_request and 'github.com' in pull_request:
            # Extract PR number from URL like https://github.com/bids-standard/bids-specification/pull/518
            parts = pull_request.split('/')
            if 'pull' in parts:
                idx = parts.index('pull')
                if idx + 1 < len(parts):
                    pr_number = parts[idx + 1].split('#')[0]

        if not pr_number:
            print(f"BEP {bep_number}: No PR linked")
            continue

        print(f"BEP {bep_number}: '{title}' -> PR #{pr_number}")

        # Check if PR schema exists
        pr_schema_dir = pr_dir / pr_number
        if not pr_schema_dir.exists():
            print(f"  PR #{pr_number} schema not found, skipping BEP {bep_number}")
            continue

        # Create BEP directory
        bep_schema_dir = bep_dir / bep_number

        # Check if we need to update
        need_update = True
        if bep_schema_dir.exists():
            # Check if BEP metadata exists and is up to date
            metadata_file = bep_schema_dir / 'BEP_METADATA.json'
            if metadata_file.exists():
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)
                    if metadata.get('pr_number') == int(pr_number):
                        # Check if PR number matches and schema exists
                        if (bep_schema_dir / 'schema.json').exists():
                            print(f"  BEP {bep_number} is already up to date")
                            need_update = False

        if need_update:
            print(f"  Syncing BEP {bep_number} from PR #{pr_number}")

            # Use datalad run if available, otherwise copy directly
            if shutil.which('datalad'):
                # Try datalad run with --explicit first
                cmd = [
                    'datalad', 'run',
                    '-m', f'Sync BEP {bep_number} schema from PR #{pr_number}',
                    '--output', f'BEPs/{bep_number}/',
                    '--explicit',
                    'cp', '-rp', f'PRs/{pr_number}/.', f'BEPs/{bep_number}/'
                ]
                result = subprocess.run(cmd, capture_output=True, text=True)

                if result.returncode != 0:
                    # If --explicit failed, check if it's due to uncommitted changes
                    if "clean dataset required" in result.stderr:
                        print(f"  Committing pending changes before BEP sync...")
                        # Commit any pending changes first
                        subprocess.run(['git', 'add', '.'])
                        subprocess.run(['git', 'commit', '-m', f'Intermediate commit before syncing BEP {bep_number}'],
                                     capture_output=True, text=True)

                        # Try datalad run again
                        result = subprocess.run(cmd, capture_output=True, text=True)

                    if result.returncode != 0:
                        error_msg = result.stderr.strip() or result.stdout.strip() or "Unknown error"
                        print(f"  DataLad run failed: {error_msg}")
                        print(f"  Falling back to direct copy...")
                        # Fall back to direct copy
                        if bep_schema_dir.exists():
                            shutil.rmtree(bep_schema_dir)
                        shutil.copytree(pr_schema_dir, bep_schema_dir)
            else:
                # Direct copy
                if bep_schema_dir.exists():
                    shutil.rmtree(bep_schema_dir)
                shutil.copytree(pr_schema_dir, bep_schema_dir)

            # Get authors count from PR metadata using cached loader
            pr_metadata_file = pr_schema_dir / 'PR_METADATA.json'
            pr_metadata = load_pr_metadata(str(pr_metadata_file))
            authors_count = pr_metadata.get('authors_count', 0)

            # Generate BEP metadata
            metadata = {
                'bep_number': bep_number,
                'title': title,
                'pr_number': int(pr_number),
                'pull_request': pull_request,
                'google_doc': bep.get('google_doc', ''),
                'status': 'review',  # Default status, could be enhanced
                'authors_count': authors_count
            }

            metadata_file = bep_schema_dir / 'BEP_METADATA.json'
            with open(metadata_file, 'w') as f:
                json.dump(metadata, f, indent=2)

            synced_count += 1

        processed_count += 1

    # Clean up BEPs that no longer have PRs
    print("\nChecking for BEPs without active PRs...")
    if bep_dir.exists():
        for bep_folder in bep_dir.iterdir():
            if not bep_folder.is_dir():
                continue

            bep_number = bep_folder.name

            # Check if this BEP still has an active PR
            metadata_file = bep_folder / 'BEP_METADATA.json'
            if metadata_file.exists():
                with open(metadata_file, 'r') as f:
                    metadata = json.load(f)
                    pr_number = str(metadata.get('pr_number', ''))

                    if pr_number and not (pr_dir / pr_number).exists():
                        print(f"  BEP {bep_number}: PR #{pr_number} no longer exists, removing BEP schema")
                        shutil.rmtree(bep_folder)

    print(f"\nSummary:")
    print(f"  BEPs processed: {processed_count}")
    print(f"  BEPs synced: {synced_count}")


if __name__ == "__main__":
    main()